<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Publication | Zhizhen&#39;s Attic</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="This page is to collect the puclications and related resources of Deep Learning in Computer Vision.It is inspired by the great resource on CSE 599 - Advanced in NLP and Awesome Deep Vision.I think the">
<meta property="og:type" content="website">
<meta property="og:title" content="Publication">
<meta property="og:url" content="http://chizhizhen.github.io/Publication/index.html">
<meta property="og:site_name" content="Zhizhen's Attic">
<meta property="og:description" content="This page is to collect the puclications and related resources of Deep Learning in Computer Vision.It is inspired by the great resource on CSE 599 - Advanced in NLP and Awesome Deep Vision.I think the">
<meta property="og:image" content="http://7xqe1l.com1.z0.glb.clouddn.com/TIP.png">
<meta property="og:image" content="http://7xqe1l.com1.z0.glb.clouddn.com/icspcc1.png">
<meta property="og:image" content="http://7xqe1l.com1.z0.glb.clouddn.com/icspcc2.png">
<meta property="og:image" content="http://7xqe1l.com1.z0.glb.clouddn.com/neuralcomputing.png">
<meta property="og:image" content="http://7xqe1l.com1.z0.glb.clouddn.com/imageclassfication.png">
<meta property="og:image" content="http://7xqe1l.com1.z0.glb.clouddn.com/objectdetection.png">
<meta property="og:image" content="http://7xqe1l.com1.z0.glb.clouddn.com/semanticsegmentation.png">
<meta property="og:image" content="http://chizhizhen.github.io/./img/publication.jpg">
<meta property="og:updated_time" content="2017-02-15T03:01:22.721Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Publication">
<meta name="twitter:description" content="This page is to collect the puclications and related resources of Deep Learning in Computer Vision.It is inspired by the great resource on CSE 599 - Advanced in NLP and Awesome Deep Vision.I think the">
<meta name="twitter:image" content="http://7xqe1l.com1.z0.glb.clouddn.com/TIP.png">
  
    <link rel="alternate" href="/atom.xml" title="Zhizhen&#39;s Attic" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Zhizhen&#39;s Attic</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Many a little make a mickle!</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/Publication">Publication</a>
        
          <a class="main-nav-link" href="/Project">Project</a>
        
          <a class="main-nav-link" href="/Misc">Misc</a>
        
          <a class="main-nav-link" href="/About">About</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://chizhizhen.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="page-undefined" class="article article-type-page" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/Publication/index.html" class="article-date">
  <time datetime="2016-01-04T13:30:29.000Z" itemprop="datePublished">2016-01-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Publication
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>This page is to collect the puclications and related resources of Deep Learning in Computer Vision.</strong><br>It is inspired by the great resource on <a href="http://homes.cs.washington.edu/~yejin/cse599.html" target="_blank" rel="external">CSE 599 - Advanced in NLP</a> and <a href="http://jiwonkim.org/awesome-deep-vision/" target="_blank" rel="external">Awesome Deep Vision.</a><br>I think the researchers in the ocean of Computer Vision should read all the paper in this <a href="http://jiwonkim.org/awesome-deep-vision/" target="_blank" rel="external">Website</a>.<br>I have also found sth interesting in <a href="https://www.reddit.com/r/machinelearning" target="_blank" rel="external">Reddit</a> for Machine Learning.</p>
<font color="#FF00FF" size="5px"> Publication </font>

<h2 id="Dual-Deep-Network-for-Visual-Tracking"><a href="#Dual-Deep-Network-for-Visual-Tracking" class="headerlink" title="Dual Deep Network for Visual Tracking"></a>Dual Deep Network for Visual Tracking</h2><h4 id="Zhizhen-Chi-Hongyang-Li-Huchuan-Lu-Minghsuan-Yang-PDF-GitHub"><a href="#Zhizhen-Chi-Hongyang-Li-Huchuan-Lu-Minghsuan-Yang-PDF-GitHub" class="headerlink" title="Zhizhen Chi, Hongyang Li, Huchuan Lu, Minghsuan Yang [PDF] [GitHub]"></a><font color="#1E90FF"><font color="#FF0000">Zhizhen Chi</font>, Hongyang Li, Huchuan Lu, Minghsuan Yang</font> [<a href="https://arxiv.org/abs/1612.06053" target="_blank" rel="external">PDF</a>] [<a href="https://github.com/chizhizhen/DNT" target="_blank" rel="external">GitHub</a>]</h4><h4 id="IEEE-Transactions-on-Image-Processing-TIP-2017"><a href="#IEEE-Transactions-on-Image-Processing-TIP-2017" class="headerlink" title="IEEE Transactions on Image Processing (TIP) 2017"></a><font color="#008000">IEEE Transactions on Image Processing (TIP) 2017</font></h4><h4 id="Submitted-to-Visual-Object-Tracking-VOT2016-Challenge-PDF"><a href="#Submitted-to-Visual-Object-Tracking-VOT2016-Challenge-PDF" class="headerlink" title="Submitted to Visual Object Tracking VOT2016 Challenge [PDF]"></a><font color="#008000">Submitted to Visual Object Tracking VOT2016 Challenge</font> [<a href="http://www.votchallenge.net/publications.html" target="_blank" rel="external">PDF</a>]</h4><p><img src="http://7xqe1l.com1.z0.glb.clouddn.com/TIP.png" width="500" height="120" alt="Pipeline"><br>In this paper, we propose a dual network to better utilize features among layers for visual tracking. It is observed that features in higher layers encode semantic context while its counterparts in lower layers are sensitive to discriminative appearance. Thus we exploit the hierarchical features in different layers of a deep model and design a dual structure to obtain better feature representation from various streams, which is rarely investigated in previous work. To leverage the robustness of our dual network, we train it with random patches measuring the similarities between the network activation and target appearance. Quantitative and qualitative evaluations on two large-scale benchmark data sets show that the proposed algorithm performs favourably against the state-of-the-arts.</p>
<h2 id="On-the-Importance-of-Network-Architecture-in-Training-Very-Deep-Neural-Networks"><a href="#On-the-Importance-of-Network-Architecture-in-Training-Very-Deep-Neural-Networks" class="headerlink" title="On the Importance of Network Architecture in Training Very Deep Neural Networks"></a>On the Importance of Network Architecture in Training Very Deep Neural Networks</h2><h4 id="Zhizhen-Chi-Hongyang-Li-Jingjing-Wang-Huchuan-Lu-PDF-GitHub"><a href="#Zhizhen-Chi-Hongyang-Li-Jingjing-Wang-Huchuan-Lu-PDF-GitHub" class="headerlink" title="Zhizhen Chi, Hongyang Li, Jingjing Wang, Huchuan Lu [PDF] [GitHub]"></a><font color="#1E90FF"><font color="#FF0000">Zhizhen Chi</font>, Hongyang Li, Jingjing Wang, Huchuan Lu</font> [<a href="http://ieeexplore.ieee.org/abstract/document/7753635/" target="_blank" rel="external">PDF</a>] [<a href="https://github.com/hli2020/torch-residual-networks" target="_blank" rel="external">GitHub</a>]</h4><h4 id="IEEE-International-Conference-on-Signal-Processing-Communications-and-Computing-ICSPCC-2016"><a href="#IEEE-International-Conference-on-Signal-Processing-Communications-and-Computing-ICSPCC-2016" class="headerlink" title="IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC), 2016"></a><font color="#008000">IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC), 2016</font></h4><p><img src="http://7xqe1l.com1.z0.glb.clouddn.com/icspcc1.png" width="800" height="200" alt="Network Architecture"><br>Very deep neural networks with hundreds or more layers have achieved significant success in a variety of vision tasks spanning from image classification, detection, to image captioning. However, simply stacking more layers in the convolution operation could suffer from the gradient vanishing problem and thus could not lower down the training loss further. The residual network [1] pushes the model’s depth to extremely deep by proposing an identity mapping plus a residual learning term and addresses the gradient back-propagation bottleneck well. In this paper, we investigate the residual module in great extent by analyzing the structure ordering of different blocks and modify them one by one to achieve lower test error on CIFAR-10 dataset. One key observation is that removing the original ReLU activation could facilitate the gradient propagation in the identity mapping path. Moreover, inspired by the ResNet block, we propose a random-jump scheme to skip some residual connections during training, i.e., lower features could jump to any subsequent layers and bypass its transformations directly to the higher level. Such an upgrade to the network structure not only saves training time but also obtains better performance.</p>
<h2 id="Image-retrieval-and-classification-on-deep-convolutional-SparkNet"><a href="#Image-retrieval-and-classification-on-deep-convolutional-SparkNet" class="headerlink" title="Image retrieval and classification on deep convolutional SparkNet"></a>Image retrieval and classification on deep convolutional SparkNet</h2><h4 id="Hongyang-Li-Peng-Su-Zhizhen-Chi-Jingjing-Wang-PDF"><a href="#Hongyang-Li-Peng-Su-Zhizhen-Chi-Jingjing-Wang-PDF" class="headerlink" title="Hongyang Li, Peng Su, Zhizhen Chi, Jingjing Wang [PDF]"></a><font color="#1E90FF">Hongyang Li, Peng Su, <font color="#FF0000">Zhizhen Chi</font>, Jingjing Wang</font> [<a href="http://ieeexplore.ieee.org/abstract/document/7753615/?reload=true" target="_blank" rel="external">PDF</a>]</h4><h4 id="IEEE-International-Conference-on-Signal-Processing-Communications-and-Computing-ICSPCC-2016-1"><a href="#IEEE-International-Conference-on-Signal-Processing-Communications-and-Computing-ICSPCC-2016-1" class="headerlink" title="IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC), 2016"></a><font color="#008000">IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC), 2016</font></h4><p><img src="http://7xqe1l.com1.z0.glb.clouddn.com/icspcc2.png" width="800" height="200" alt="Pipeline"><br>Image retrieval and classification are hot topics in computer vision and have attracted great attention nowadays with the emergence of large-scale data. We propose a new scheme to use both deep learning models and large-scale computing platform and jointly learn powerful feature representations in image classification and retrieval. We achieve a superior performance on the ImageNet dataset, where the framework is easy to be embedded for daily user experience. First we conduct the classification task using deep convolutional neural networks with several novel techniques, including batch normalization and multi-crop testing to obtain a better performance. Then we transfer the network’s knowledge to image retrieval task by comparing the feature codebook of the query image with those feature database extracted from the deep model. Such a search pipeline is implemented in a MapReduce framework on the Spark platform, which is suitable for large-scale and real-time data processing. At last, the system outputs to users some textual information of the predicted object searching from Internet as well as similar images from the retrieval stage, making our work a real application.</p>
<h2 id="CNN-for-saliency-detection-with-low-level-feature-integration"><a href="#CNN-for-saliency-detection-with-low-level-feature-integration" class="headerlink" title="CNN for saliency detection with low-level feature integration"></a>CNN for saliency detection with low-level feature integration</h2><h4 id="Hongyang-Li-Jiang-Chen-Zhizhen-Chi-Huchuan-Lu-PDF"><a href="#Hongyang-Li-Jiang-Chen-Zhizhen-Chi-Huchuan-Lu-PDF" class="headerlink" title="Hongyang Li, Jiang Chen, Zhizhen Chi, Huchuan Lu [PDF]"></a><font color="#1E90FF">Hongyang Li, Jiang Chen, <font color="#FF0000">Zhizhen Chi</font>, Huchuan Lu</font> [<a href="http://www.sciencedirect.com/science/article/pii/S0925231216314710" target="_blank" rel="external">PDF</a>]</h4><h4 id="Neurocomputing-Volume-226-22-February-2017-Pages-212–220"><a href="#Neurocomputing-Volume-226-22-February-2017-Pages-212–220" class="headerlink" title="Neurocomputing, Volume 226, 22 February 2017, Pages 212–220"></a><font color="#008000">Neurocomputing, Volume 226, 22 February 2017, Pages 212–220</font></h4><p><img src="http://7xqe1l.com1.z0.glb.clouddn.com/neuralcomputing.png" width="800" height="200" alt="Pipeline"><br>Feature matters. In this paper, a novel deep neural network framework integrated with low-level features for salient object detection is proposed for complex images. We utilise the advantage of convolutional neural networks to automatically learn the high-level features that capture the structured information and semantic context in the image. In order to better adapt a CNN model into the saliency task, we redesign the network architecture based on typical saliency datasets, which is relatively small-scale compared to ImageNet. Several low-level features are extracted, which can effectively capture contrast and spatial information in the salient regions, and incorporated to compensate with the learned high-level features at the output of the very last fully connected layer. The concatenated feature vector is further fed into a hinge-loss SVM detector in a joint discriminative learning manner and the final saliency score of each region within the bounding box is obtained by the linear combination of the detector’s weights. Experiments on three challenging benchmarks demonstrate our algorithm to be effective and superior than most low-level oriented state-of-the-arts in terms of precision-recall curves, F-measure and mean absolute errors. Moreover, a series of ablation studies are conducted to verify our algorithm’s simplicity and efficiency from different aspects.</p>
<p><font color="#FF00FF" size="5px"> Visual Tracking </font><br>Review on Visual Tracking: <a href="http://www.ppgia.pucpr.br/~alceu/pdi/Video%20Segmentation%20and%20Tracking/Yilmaz.pdf" target="_blank" rel="external">1</a>, <a href="http://winsty.net/papers/diagnose.pdf" target="_blank" rel="external">2</a>(Recommended), <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6671560" target="_blank" rel="external">3</a>. The followings are state-of-the-art trackers. Last updated on 28/01/2016.</p>
<ul>
<li><p>Generative Model<br>1) D. A. Ross, J. Lim, R. Lin, and M. Yang. <a href="http://www.cs.toronto.edu/~dross/ivt/" target="_blank" rel="external">IVT</a> In IJCV, 2008<br> 2) X. Mei and H. Ling. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.204.2954&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">L1_Min</a> In CVPR, 2009<br> 2) W. Zhong, H. Lu, and M. Yang. <a href="http://faculty.ucmerced.edu/mhyang/papers/tip14_scm.pdf" target="_blank" rel="external">SCM</a> In TIP, 2014<br> 3) Jia, Xu, Lu, Huchuan, and Yang, Ming-Hsuan. <a href="http://faculty.ucmerced.edu/mhyang/project/cvpr12_jia_project.files/cvpr2012_xu_1525.pdf" target="_blank" rel="external">ASLA</a> In CVPR, 2012<br> 4) J. Kwon and K. M. Lee. <a href="http://cv.snu.ac.kr/publication/conf/2010/VTD_CVPR2010.pdf" target="_blank" rel="external">VTD</a> In CVPR, 2010<br> 5) J. Kwon and K. M. Lee. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.398.4969&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">VTS</a> In ICCV, 2011</p>
</li>
<li><p>Discrimitive Model<br>1) B. Babenko, M.-H. Yang, and S. Belongie. <a href="http://vision.ucsd.edu/~bbabenko/data/miltrack_cvpr09.pdf" target="_blank" rel="external">MIL</a> In TPAMI, 2011<br> 2) H. Grabner, C. Leistner, and H. Bischof. <a href="http://www.vision.ee.ethz.ch/boostingTrackers/Grabner2008Semi-supervisedOn-lineboosting.pdf" target="_blank" rel="external">Semi-supervised</a> In ECCV, 2008<br> 3) Z. Kalal, K. Mikolajczyk, and J. Matas. <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6104061" target="_blank" rel="external">TLD</a> In TPAMI, 2012<br> 4) H. Grabner, C. Leistner, and H. Bischof. <a href="http://cs-people.bu.edu/jmzhang/MEEM/MEEM.html" target="_blank" rel="external">MEEM</a> In ECCV, 2014<br> 5) S. Hare, A. Saffari, and P. H. S. Torr. <a href="http://core.ac.uk/download/files/89/241181.pdf" target="_blank" rel="external">Struck</a> In ICCV, 2011<br> 6) X. Ren and J. Malik. <a href="https://homes.cs.washington.edu/~xren/publication/xren_cvpr07_tracking.pdf" target="_blank" rel="external">CRFs</a> In CVPR, 2007<br> 7) H. Grabner, C. Leistner, and H. Bischof. <a href="http://www.icg.tugraz.at/Members/hgrabner/pub_hgrabner/Grabner2008Semi-supervisedOn-lineboosting.pdf" target="_blank" rel="external">Online-Boosting</a> In ECCV, 2008<br> 8) T. B. Dinh, N. Vo, and G. Medioni. <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5995733" target="_blank" rel="external">CXT</a> In CVPR, 2011</p>
</li>
<li><p>Correlation Filters<br>1) D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui. <a href="http://www.cs.colostate.edu/~vision/publications/bolme_cvpr10.pdf" target="_blank" rel="external">MOSSE</a> In CVPR, 2010<br> 2) J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. <a href="http://home.isr.uc.pt/~pedromartins/Publications/henriques_eccv2012.pdf" target="_blank" rel="external">CSK</a> In ECCV, 2012<br> 3) J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. <a href="http://home.isr.uc.pt/~henriques/circulant/" target="_blank" rel="external">KCF</a> In TPAMI, 2015<br> 4) M. Danelljan, F. S. Khan, M. Felsberg, and J. van de Weijer. <a href="http://www.cvc.uab.es/lamp/wp-content/uploads/Projects/pdfs/2014CVPR_tracking.pdf" target="_blank" rel="external">ColorName</a> In CVPR, 2014<br> 5) K. Zhang, L. Zhang, Q. Liu, D. Zhang, and M.-H. Yang. <a href="http://www4.comp.polyu.edu.hk/~cslzhang/paper/conf/STC_eccv14.pdf" target="_blank" rel="external">STC</a> In ECCV, 2014</p>
</li>
<li><p>Deep Learning based Trackers<br>1) N. Wang and D. Yeung. <a href="http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf" target="_blank" rel="external">DLT</a> In NIPS, 2013<br> 2) H. Li, Y. Li, and F. Porikli. <a href="http://www.bmva.org/bmvc/2014/papers/paper028/" target="_blank" rel="external">DeepTrack</a> In BMVC, 2014<br> 3) C. Ma and M. Yang. <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Ma_Hierarchical_Convolutional_Features_ICCV_2015_paper.pdf" target="_blank" rel="external">HCF</a> In ICCV, 2015<br> 4) Lijun Wang, Wanli Ouyang, Xiaogang Wang, and Huchuan Lu. <a href="http://scott89.github.io/FCNT/" target="_blank" rel="external">FCNT</a> In ICCV, 2015</p>
</li>
</ul>
<p><font color="#FF00FF" size="5px"> Deep Learning </font></p>
<ul>
<li><p>Image Classification<br><img src="http://7xqe1l.com1.z0.glb.clouddn.com/imageclassfication.png" alt="(from Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.)"><br>1) A. Krizhevsky, I. Sutskever, and G. E. Hinton. <a href="http://papers.nips.cc/paper/4824-imagenet-classification-w" target="_blank" rel="external">AlexNet</a> In NIPS, 2012<br> 2) K. Simonyan and A. Zisserman. <a href="Very deep convolutional
networks for large-scale image recognition.">VGG</a> In CoRR, 2014<br> 3) Many authors…<a href="http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf" target="_blank" rel="external">GoogleNet</a> In CVPR, 2015</p>
</li>
<li><p>Object Detection<br><img src="http://7xqe1l.com1.z0.glb.clouddn.com/objectdetection.png" alt="(from Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, arXiv:1506.01497.)"><br>1) P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. <a href="https://www.researchgate.net/publication/259441043_OverFeat_Integrated_Recognition_Localization_and_Detection_using_Convolutional_Networks" target="_blank" rel="external">OverFeat</a> In ICLR, 2014<br> 2) R. Girshick, J. Donahue, T. Darrell, and J. Malik. <a href="http://www.cs.berkeley.edu/~rbg/papers/r-cnn-cvpr.pdf" target="_blank" rel="external">RCNN</a> In CVPR, 2014<br> 3) R. Girshick, <a href="http://arxiv.org/pdf/1504.08083v2.pdf" target="_blank" rel="external">Fast-RCNN</a> In ICCV, 2015<br> 4) Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. <a href="http://arxiv.org/pdf/1506.01497v3.pdf" target="_blank" rel="external">Faster-RCNN</a></p>
</li>
<li><p>Semantic Segmentation<br><img src="http://7xqe1l.com1.z0.glb.clouddn.com/semanticsegmentation.png" alt="(from Jifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640.)"><br>1) J. Long, E. Shelhamer and T.Darrell. <a href="http://www.cs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank" rel="external">FCT</a> In CVPR, 2015</p>
</li>
</ul>
<p><font color="#FF00FF" size="5px"> Object Proposal </font></p>
<ul>
<li><p>Window based methods<br>1) Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. What is an object? <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5540226&amp;tag=1" target="_blank" rel="external">Objectness (O)</a> In CVPR, 2010.<br> 2) Esa Rahtu, Juho Kannala, and Matthew B. Blaschko. Learning a category independent object detection cascade. <a href="https://www.robots.ox.ac.uk/~vgg/publications/2011/Rahtu11/rahtu11.pdf" target="_blank" rel="external">Rathu2011 (R1)</a> In ICCV, 2011.<br> 3) Ming-Ming Cheng, Ziming Zhang, Wen-Yan Lin, and Philip H. S. Torr. BING: binarized normed gradients for objectness estimation at 300fps. <a href="https://www.robots.ox.ac.uk/~vgg/rg/papers/14cvprObjectnessBING.pdf" target="_blank" rel="external">BING (B)</a> In CVPR, 2014.<br> 4) C. Lawrence Zitnick and Piotr Dollár. Edge boxes: Locating object proposals from edges. <a href="http://research.microsoft.com/pubs/220569/ZitnickDollarECCV14edgeBoxes.pdf" target="_blank" rel="external">EdgeBox (EB)</a> In ECCV, 2014.</p>
</li>
<li><p>Region based methods<br>1) João Carreira and Cristian Sminchisescu. Constrained parametric min-cuts for automatic object segmentation. <a href="http://www.eecs.berkeley.edu/~carreira/papers/cvpr2010_2.pdf" target="_blank" rel="external">CPMC (C)</a> In CVPR, 2010.<br> 2) Pablo Andrés Arbeláez, Jordi Pont-Tuset, Jonathan T. Barron, Ferran Marqués, and Jitendra Malik. Multiscale combinatorial grouping. <a href="http://www.cs.berkeley.edu/~barron/ArbelaezCVPR2014.pdf" target="_blank" rel="external">MCG (M)</a> In CVPR, 2014.<br> 3) Koen E. A. van de Sande, Jasper R. R. Uijlings, Theo Gevers, and ArnoldW.M. Smeulders. Segmentation as selective search for object recognition. <a href="https://www.robots.ox.ac.uk/~vgg/rg/papers/sande_iccv11.pdf" target="_blank" rel="external">SelectiveSearch (SS)</a> In ICCV, 2011.<br> 4) Santiago Manen, Matthieu Guillaumin, and Luc J. Van Gool. Prime object proposals with randomized prim’s algorithm. <a href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Manen_Prime_Object_Proposals_2013_ICCV_paper.pdf" target="_blank" rel="external">RandomizedPrim (RP)</a> In ICCV, 2013.<br> 5) Philipp Krähenbühl and Vladlen Koltun. Geodesic object proposals. <a href="http://www.philkr.net/home/gop" target="_blank" rel="external">Geodesic (G)</a> In ECCV, 2014<br> 6) Ahmad Humayun, Fuxin Li, and James M. Rehg. Geodesic object proposals. <a href="http://cpl.cc.gatech.edu/projects/RIGOR/pubs/humayun_CVPR_2014_rigor.pdf" target="_blank" rel="external">RIGOR (RI)</a> In CVPR, 2014</p>
</li>
</ul>
<p><img src="./img/publication.jpg" alt="让我中一篇顶会吧！"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://chizhizhen.github.io/Publication/index.html" data-id="ciz8gbvv30009oofa1wh2t4mn" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithms/">Algorithms</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Caffe/">Caffe</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Git/">Git</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Image-Processing/">Image Processing</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode-OJ/">LeetCode OJ</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Object-Detection/">Object Detection</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SQL/">SQL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Text-Editor/">Text Editor</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Ubuntu/">Ubuntu</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Academic-Research/">Academic Research</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithms/">Algorithms</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Languages/">Languages</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LeetCode-OJ/">LeetCode OJ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Projects/">Projects</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Softwares/">Softwares</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sources/">Sources</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/杂记/">杂记</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Academic-Research/" style="font-size: 20px;">Academic Research</a> <a href="/tags/Algorithms/" style="font-size: 10px;">Algorithms</a> <a href="/tags/Languages/" style="font-size: 12.5px;">Languages</a> <a href="/tags/LeetCode-OJ/" style="font-size: 10px;">LeetCode OJ</a> <a href="/tags/Projects/" style="font-size: 10px;">Projects</a> <a href="/tags/Softwares/" style="font-size: 17.5px;">Softwares</a> <a href="/tags/Sources/" style="font-size: 15px;">Sources</a> <a href="/tags/杂记/" style="font-size: 12.5px;">杂记</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/10/19/Window-Data-Layer-in-Caffe/">Window Data Layer in Caffe</a>
          </li>
        
          <li>
            <a href="/2016/06/20/Vehicle-Detection-Tracking-with-some-auxiliary-functions/">Vehicle Detection, Tracking with some auxiliary functions</a>
          </li>
        
          <li>
            <a href="/2016/05/19/新版caffe的MATLAB接口/">新版caffe的MATLAB接口</a>
          </li>
        
          <li>
            <a href="/2016/05/06/Git时如何解决出现：Commit-your-changes-or-stash-them-before-you-can-merge/">Git时如何解决出现：Commit your changes or stash them before you can merge</a>
          </li>
        
          <li>
            <a href="/2016/05/06/Python-os-path-模块/">Python os.path 模块</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    <div class="widget tag">
<h3 class="title">Friends</h3>
<ul class="entry">
<li><a href="http://www.ee.cuhk.edu.hk/~yangli/" title="Li Hongyang">Hongyang Li</a></li>
<li><a href="https://sites.google.com/site/jingjingwanghomepage/" title="Wang Jingjing">Jingjing Wang</a></li>
<li><a href="http://ice.dlut.edu.cn/lu/index.html" title="Lu Huchuan">Huchuan Lu</a></li>
<li><a href="http://faculty.ucmerced.edu/mhyang/" title="Yang Ming-Hsuan">Ming-Hsuan Yang</a></li>
<li><a href="http://karpathy.github.io/" title="Andrej Karpathy">Andrej Karpathy</a></li>
<li><a href="http://www.theclevercarrot.com/" title="The Clever Carrot">The Clever Carrot</a></li>
</ul>
</div>
  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Chi Zhizhen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/Publication" class="mobile-nav-link">Publication</a>
  
    <a href="/Project" class="mobile-nav-link">Project</a>
  
    <a href="/Misc" class="mobile-nav-link">Misc</a>
  
    <a href="/About" class="mobile-nav-link">About</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>